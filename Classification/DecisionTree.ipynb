{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          RI     Na    Mg    Al     Si     K    Ca    Ba   Fe\n",
      "0    1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.00  0.0\n",
      "1    1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.00  0.0\n",
      "2    1.51618  13.53  3.55  1.54  72.99  0.39  7.78  0.00  0.0\n",
      "3    1.51766  13.21  3.69  1.29  72.61  0.57  8.22  0.00  0.0\n",
      "4    1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.00  0.0\n",
      "..       ...    ...   ...   ...    ...   ...   ...   ...  ...\n",
      "209  1.51623  14.14  0.00  2.88  72.61  0.08  9.18  1.06  0.0\n",
      "210  1.51685  14.92  0.00  1.99  73.06  0.00  8.40  1.59  0.0\n",
      "211  1.52065  14.36  0.00  2.02  73.42  0.00  8.44  1.64  0.0\n",
      "212  1.51651  14.38  0.00  1.94  73.61  0.00  8.48  1.57  0.0\n",
      "213  1.51711  14.23  0.00  2.08  73.36  0.00  8.62  1.67  0.0\n",
      "\n",
      "[214 rows x 9 columns]\n",
      "     Type\n",
      "0       1\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "..    ...\n",
      "209     7\n",
      "210     7\n",
      "211     7\n",
      "212     7\n",
      "213     7\n",
      "\n",
      "[214 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn.tree\n",
    "data=pd.read_csv(\"D:\\Coursera\\GlassClassification.csv\")\n",
    "#print(data)\n",
    "data.shape\n",
    "#Separating Indepent data and Target data\n",
    "X=data.drop(['Type'],axis=1)\n",
    "print(X)\n",
    "y=data[['Type']]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score,jaccard_score,f1_score,log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average Jaccard Score is : 0.5729896006549998\n",
      "The average F1 Score is : 0.7186046511627907\n",
      "The average accuracy is : 0.7186046511627907\n"
     ]
    }
   ],
   "source": [
    "model=DecisionTreeClassifier()\n",
    "n=10\n",
    "totalAccuracy=0\n",
    "totalJaccardScore=0\n",
    "totalF1Score=0\n",
    "totalLogLoss=0\n",
    "\n",
    "for i in range(n):\n",
    "    \n",
    "    #splitting the model\n",
    "    xTrain,xTest,yTrain,yTest=train_test_split(X,y,test_size=0.2,random_state=1)\n",
    "    \n",
    "    #Train the model\n",
    "    model.fit(xTrain,yTrain)\n",
    "    \n",
    "    #Predict the outputs\n",
    "    yPredict=model.predict(xTest)\n",
    "    \n",
    "    #Generating the Evaluation score\n",
    "        \n",
    "        #Accuracy Measure\n",
    "    immAccuracy=accuracy_score(yTest,yPredict)\n",
    "    totalAccuracy=totalAccuracy+immAccuracy\n",
    "        \n",
    "        #Jaccard Score Measure\n",
    "    immJaccardScore=jaccard_score(yTest,yPredict,average='macro')\n",
    "    totalJaccardScore=totalJaccardScore+immJaccardScore\n",
    "        \n",
    "        #F1 Score Measure\n",
    "    immF1Score=f1_score(yTest,yPredict,average='micro')\n",
    "    totalF1Score=totalF1Score+immF1Score\n",
    "    \n",
    "    '''    #Logloass Measure\n",
    "    immLogLoss=log_loss(yTest,yPredict,eps=1e-15,normalize=True)\n",
    "    totalLogLoss=totalLogLoss+immLogLoss'''\n",
    "    \n",
    "    #print(immAccuracy)\n",
    "    #print(immJaccardScore)\n",
    "    #print(immF1Score)\n",
    "    #print(totalLogLoss)\n",
    "    #model.get_depth()\n",
    "    #print(model.get_n_leaves())\n",
    "\n",
    "# Taking the average of Evalation Scores\n",
    "avgAccuracy=totalAccuracy/n\n",
    "avgJaccardScore=totalJaccardScore/n\n",
    "avgF1Score=totalF1Score/n\n",
    "#avgLogLoss=totalLogLoss/n\n",
    "\n",
    "print(\"The average Jaccard Score is :\", avgJaccardScore)\n",
    "print(\"The average F1 Score is :\", avgF1Score)\n",
    "#print(\"The average Log Loss Score is :\", avgLogLoss)\n",
    "print(\"The average accuracy is :\", avgAccuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
